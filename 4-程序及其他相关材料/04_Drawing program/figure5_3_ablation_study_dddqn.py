#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DDDQN-PER æ¶ˆèå®éªŒåˆ†æ
åˆ†æDuelingã€Double DQNã€PERä¸‰ä¸ªç»„ä»¶çš„ç‹¬ç«‹è´¡çŒ®
"""

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import argparse
import logging
from datetime import datetime
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque, namedtuple
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent.parent / "01_source_code"))

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Experience:
    """ç»éªŒå…ƒç»„"""
    def __init__(self, state, action, reward, next_state, done):
        self.state = state
        self.action = action
        self.reward = reward
        self.next_state = next_state
        self.done = done

class StandardReplayBuffer:
    """æ ‡å‡†ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆéä¼˜å…ˆçº§ï¼‰"""
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        experiences = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([e.state for e in experiences]).to(device)
        actions = torch.LongTensor([e.action for e in experiences]).to(device)
        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)
        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)
        dones = torch.BoolTensor([e.done for e in experiences]).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class PrioritizedReplayBuffer:
    """ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001):
        self.capacity = capacity
        self.alpha = alpha
        self.beta = beta
        self.beta_increment = beta_increment
        
        self.buffer = []
        self.priorities = np.zeros((capacity,), dtype=np.float32)
        self.position = 0
        self.size = 0
    
    def push(self, state, action, reward, next_state, done):
        max_priority = self.priorities.max() if self.buffer else 1.0
        
        experience = Experience(state, action, reward, next_state, done)
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
        else:
            self.buffer[self.position] = experience
        
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size):
        if self.size < batch_size:
            return None, None, None
        
        priorities = self.priorities[:self.size]
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()
        
        indices = np.random.choice(self.size, batch_size, p=probabilities)
        experiences = [self.buffer[idx] for idx in indices]
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        total = self.size
        weights = (total * probabilities[indices]) ** (-self.beta)
        weights /= weights.max()
        weights = torch.FloatTensor(weights).to(device)
        
        states = torch.FloatTensor([e.state for e in experiences]).to(device)
        actions = torch.LongTensor([e.action for e in experiences]).to(device)
        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)
        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)
        dones = torch.BoolTensor([e.done for e in experiences]).to(device)
        
        return (states, actions, rewards, next_states, dones), indices, weights
    
    def update_priorities(self, indices, priorities):
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority + 1e-6
        
        self.beta = min(1.0, self.beta + self.beta_increment)
    
    def __len__(self):
        return self.size

class StandardQNetwork(nn.Module):
    """æ ‡å‡†DQNç½‘ç»œï¼ˆéDuelingï¼‰"""
    def __init__(self, state_size, action_size, hidden_sizes=[256, 256]):
        super(StandardQNetwork, self).__init__()
        
        layers = []
        input_size = state_size
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(input_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            input_size = hidden_size
        
        layers.append(nn.Linear(hidden_sizes[-1], action_size))
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        return self.network(state)

class DuelingQNetwork(nn.Module):
    """Dueling DQNç½‘ç»œ"""
    def __init__(self, state_size, action_size, hidden_sizes=[256, 256]):
        super(DuelingQNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾å±‚
        self.feature_layers = nn.Sequential(
            nn.Linear(state_size, hidden_sizes[0]),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_sizes[0], hidden_sizes[1]),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # ä»·å€¼æµ
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        # ä¼˜åŠ¿æµ
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 128),
            nn.ReLU(),
            nn.Linear(128, action_size)
        )
    
    def forward(self, state):
        features = self.feature_layers(state)
        
        value = self.value_stream(features)
        advantage = self.advantage_stream(features)
        
        # Duelingæ¶æ„ï¼šQ(s,a) = V(s) + A(s,a) - mean(A(s,a'))
        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)
        
        return q_values

class AblationAgent:
    """æ¶ˆèå®éªŒæ™ºèƒ½ä½“åŸºç±»"""
    def __init__(self, state_size, action_size, config):
        self.state_size = state_size
        self.action_size = action_size
        self.config = config
        
        # è¶…å‚æ•°
        self.learning_rate = config.get('learning_rate', 1e-4)
        self.gamma = config.get('gamma', 0.99)
        self.epsilon = config.get('epsilon', 1.0)
        self.epsilon_min = config.get('epsilon_min', 0.01)
        self.epsilon_decay = config.get('epsilon_decay', 0.995)
        self.batch_size = config.get('batch_size', 64)
        self.target_update_frequency = config.get('target_update_frequency', 1000)
        
        # è®­ç»ƒç»Ÿè®¡
        self.step_count = 0
        self.loss_history = []
        self.reward_history = []
    
    def select_action(self, state):
        """Îµ-è´ªå©ªåŠ¨ä½œé€‰æ‹©"""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.action_size)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
        return q_values.argmax().item()
    
    def update_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())

class VanillaDQNAgent(AblationAgent):
    """æ¶ˆèå˜ä½“1ï¼šåŸºç¡€DQNï¼ˆæ— Duelingã€æ— Doubleã€æ— PERï¼‰"""
    def __init__(self, state_size, action_size, config):
        super().__init__(state_size, action_size, config)
        
        # æ ‡å‡†Qç½‘ç»œ
        self.q_network = StandardQNetwork(state_size, action_size).to(device)
        self.target_network = StandardQNetwork(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        # æ ‡å‡†ç»éªŒå›æ”¾
        self.memory = StandardReplayBuffer(config.get('buffer_size', 100000))
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
        
        logger.info("åˆå§‹åŒ–åŸºç¡€DQNæ™ºèƒ½ä½“")
    
    def store_experience(self, state, action, reward, next_state, done):
        self.memory.push(state, action, reward, next_state, done)
    
    def learn(self):
        if len(self.memory) < self.batch_size:
            return None
        
        # æ ‡å‡†é‡‡æ ·
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        # å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # ç›®æ ‡Qå€¼ï¼ˆæ ‡å‡†DQNï¼‰
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # æŸå¤±è®¡ç®—
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.step_count += 1
        if self.step_count % self.target_update_frequency == 0:
            self.update_target_network()
        
        # è®°å½•
        self.loss_history.append(loss.item())
        
        return loss.item()

class DuelingDQNAgent(AblationAgent):
    """æ¶ˆèå˜ä½“2ï¼šDueling DQNï¼ˆæœ‰Duelingã€æ— Doubleã€æ— PERï¼‰"""
    def __init__(self, state_size, action_size, config):
        super().__init__(state_size, action_size, config)
        
        # Dueling Qç½‘ç»œ
        self.q_network = DuelingQNetwork(state_size, action_size).to(device)
        self.target_network = DuelingQNetwork(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        # æ ‡å‡†ç»éªŒå›æ”¾
        self.memory = StandardReplayBuffer(config.get('buffer_size', 100000))
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
        
        logger.info("åˆå§‹åŒ–Dueling DQNæ™ºèƒ½ä½“")
    
    def store_experience(self, state, action, reward, next_state, done):
        self.memory.push(state, action, reward, next_state, done)
    
    def learn(self):
        if len(self.memory) < self.batch_size:
            return None
        
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        self.step_count += 1
        if self.step_count % self.target_update_frequency == 0:
            self.update_target_network()
        
        self.loss_history.append(loss.item())
        return loss.item()

class DoubleDQNAgent(AblationAgent):
    """æ¶ˆèå˜ä½“3ï¼šDouble DQNï¼ˆæ— Duelingã€æœ‰Doubleã€æ— PERï¼‰"""
    def __init__(self, state_size, action_size, config):
        super().__init__(state_size, action_size, config)
        
        # æ ‡å‡†Qç½‘ç»œ
        self.q_network = StandardQNetwork(state_size, action_size).to(device)
        self.target_network = StandardQNetwork(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        # æ ‡å‡†ç»éªŒå›æ”¾
        self.memory = StandardReplayBuffer(config.get('buffer_size', 100000))
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
        
        logger.info("åˆå§‹åŒ–Double DQNæ™ºèƒ½ä½“")
    
    def store_experience(self, state, action, reward, next_state, done):
        self.memory.push(state, action, reward, next_state, done)
    
    def learn(self):
        if len(self.memory) < self.batch_size:
            return None
        
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Double DQNï¼šä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼
        with torch.no_grad():
            next_actions = self.q_network(next_states).argmax(1)
            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        self.step_count += 1
        if self.step_count % self.target_update_frequency == 0:
            self.update_target_network()
        
        self.loss_history.append(loss.item())
        return loss.item()

class PERDQNAgent(AblationAgent):
    """æ¶ˆèå˜ä½“4ï¼šPER DQNï¼ˆæ— Duelingã€æ— Doubleã€æœ‰PERï¼‰"""
    def __init__(self, state_size, action_size, config):
        super().__init__(state_size, action_size, config)
        
        # æ ‡å‡†Qç½‘ç»œ
        self.q_network = StandardQNetwork(state_size, action_size).to(device)
        self.target_network = StandardQNetwork(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        # ä¼˜å…ˆç»éªŒå›æ”¾
        self.memory = PrioritizedReplayBuffer(config.get('buffer_size', 100000))
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
        
        logger.info("åˆå§‹åŒ–PER DQNæ™ºèƒ½ä½“")
    
    def store_experience(self, state, action, reward, next_state, done):
        self.memory.push(state, action, reward, next_state, done)
    
    def learn(self):
        if len(self.memory) < self.batch_size:
            return None
        
        # ä¼˜å…ˆé‡‡æ ·
        batch_data, indices, weights = self.memory.sample(self.batch_size)
        if batch_data is None:
            return None
        
        states, actions, rewards, next_states, dones = batch_data
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # TDè¯¯å·®
        td_errors = torch.abs(current_q_values.squeeze() - target_q_values).detach().cpu().numpy()
        
        # åŠ æƒæŸå¤±
        loss = (weights * F.mse_loss(current_q_values.squeeze(), target_q_values, reduction='none')).mean()
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        # æ›´æ–°ä¼˜å…ˆçº§
        self.memory.update_priorities(indices, td_errors)
        
        self.step_count += 1
        if self.step_count % self.target_update_frequency == 0:
            self.update_target_network()
        
        self.loss_history.append(loss.item())
        return loss.item()

class DDDQNPERAgent(AblationAgent):
    """å®Œæ•´DDDQN-PERï¼ˆæœ‰Duelingã€æœ‰Doubleã€æœ‰PERï¼‰"""
    def __init__(self, state_size, action_size, config):
        super().__init__(state_size, action_size, config)
        
        # Dueling Qç½‘ç»œ
        self.q_network = DuelingQNetwork(state_size, action_size).to(device)
        self.target_network = DuelingQNetwork(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        # ä¼˜å…ˆç»éªŒå›æ”¾
        self.memory = PrioritizedReplayBuffer(config.get('buffer_size', 100000))
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
        
        logger.info("åˆå§‹åŒ–å®Œæ•´DDDQN-PERæ™ºèƒ½ä½“")
    
    def store_experience(self, state, action, reward, next_state, done):
        self.memory.push(state, action, reward, next_state, done)
    
    def learn(self):
        if len(self.memory) < self.batch_size:
            return None
        
        batch_data, indices, weights = self.memory.sample(self.batch_size)
        if batch_data is None:
            return None
        
        states, actions, rewards, next_states, dones = batch_data
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Double DQN + Dueling
        with torch.no_grad():
            next_actions = self.q_network(next_states).argmax(1)
            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # TDè¯¯å·®
        td_errors = torch.abs(current_q_values.squeeze() - target_q_values).detach().cpu().numpy()
        
        # åŠ æƒæŸå¤±
        loss = (weights * F.mse_loss(current_q_values.squeeze(), target_q_values, reduction='none')).mean()
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        # æ›´æ–°ä¼˜å…ˆçº§
        self.memory.update_priorities(indices, td_errors)
        
        self.step_count += 1
        if self.step_count % self.target_update_frequency == 0:
            self.update_target_network()
        
        self.loss_history.append(loss.item())
        return loss.item()

class AblationExperiment:
    """æ¶ˆèå®éªŒç®¡ç†å™¨"""
    def __init__(self, output_dir="py/ablation_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰æ¶ˆèå˜ä½“
        self.variants = {
            'Vanilla_DQN': {
                'agent_class': VanillaDQNAgent,
                'description': 'åŸºç¡€DQNï¼ˆæ— å¢å¼ºç»„ä»¶ï¼‰',
                'components': []
            },
            'Dueling_DQN': {
                'agent_class': DuelingDQNAgent,
                'description': 'Dueling DQNï¼ˆä»…Duelingç½‘ç»œï¼‰',
                'components': ['Dueling']
            },
            'Double_DQN': {
                'agent_class': DoubleDQNAgent,
                'description': 'Double DQNï¼ˆä»…Double Q-learningï¼‰',
                'components': ['Double']
            },
            'PER_DQN': {
                'agent_class': PERDQNAgent,
                'description': 'PER DQNï¼ˆä»…ä¼˜å…ˆç»éªŒå›æ”¾ï¼‰',
                'components': ['PER']
            },
            'DDDQN_PER': {
                'agent_class': DDDQNPERAgent,
                'description': 'å®Œæ•´DDDQN-PERï¼ˆå…¨éƒ¨ç»„ä»¶ï¼‰',
                'components': ['Dueling', 'Double', 'PER']
            }
        }
        
        self.results = {}
    
    def run_single_variant(self, variant_name, config, episodes=100):
        """è¿è¡Œå•ä¸ªæ¶ˆèå˜ä½“"""
        logger.info(f"å¼€å§‹è®­ç»ƒ {variant_name}: {self.variants[variant_name]['description']}")
        
        # åˆ›å»ºç®€åŒ–ç¯å¢ƒï¼ˆç”¨äºå¿«é€ŸéªŒè¯ï¼‰
        from code.evves_env import EVVESEnv
        
        # ç®€åŒ–çš„ç¯å¢ƒé…ç½®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        env_config = {
            'episode_length': 96,  # 1å¤©æ•°æ®
            'soc_init_range': (0.3, 0.7),
            'action_space_type': 'discrete',
            'reward_function': 'comprehensive'
        }
        
        # å‡è®¾ç¯å¢ƒæ•°æ®
        state_size = 15
        action_size = 21
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent_class = self.variants[variant_name]['agent_class']
        agent = agent_class(state_size, action_size, config)
        
        # è®­ç»ƒè®°å½•
        episode_rewards = []
        episode_losses = []
        convergence_data = []
        
        for episode in range(episodes):
            # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’ï¼ˆç®€åŒ–ç‰ˆï¼‰
            state = np.random.randn(state_size)
            episode_reward = 0
            episode_loss = []
            
            for step in range(96):  # 1å¤©
                # é€‰æ‹©åŠ¨ä½œ
                action = agent.select_action(state)
                
                # æ¨¡æ‹Ÿç¯å¢ƒå“åº”
                next_state = np.random.randn(state_size)
                reward = np.random.normal(-0.5, 0.2)  # æ¨¡æ‹Ÿå¥–åŠ±
                done = (step == 95)
                
                # å­˜å‚¨ç»éªŒ
                agent.store_experience(state, action, reward, next_state, done)
                
                # å­¦ä¹ 
                if len(agent.memory) > agent.batch_size:
                    loss = agent.learn()
                    if loss is not None:
                        episode_loss.append(loss)
                
                episode_reward += reward
                state = next_state
            
            # æ›´æ–°æ¢ç´¢ç‡
            agent.update_epsilon()
            
            # è®°å½•ç»“æœ
            episode_rewards.append(episode_reward)
            avg_loss = np.mean(episode_loss) if episode_loss else 0
            episode_losses.append(avg_loss)
            
            # è®°å½•æ”¶æ•›æ•°æ®
            if episode % 10 == 0:
                recent_rewards = episode_rewards[-10:]
                convergence_data.append({
                    'episode': episode,
                    'avg_reward': np.mean(recent_rewards),
                    'avg_loss': avg_loss,
                    'epsilon': agent.epsilon
                })
            
            if episode % 20 == 0:
                logger.info(f"{variant_name} - Episode {episode}: "
                          f"Reward={episode_reward:.3f}, "
                          f"Loss={avg_loss:.6f}, "
                          f"Îµ={agent.epsilon:.3f}")
        
        # æ±‡æ€»ç»“æœ
        results = {
            'variant_name': variant_name,
            'description': self.variants[variant_name]['description'],
            'components': self.variants[variant_name]['components'],
            'final_performance': {
                'avg_reward_last_20': np.mean(episode_rewards[-20:]),
                'std_reward_last_20': np.std(episode_rewards[-20:]),
                'best_reward': np.max(episode_rewards),
                'final_epsilon': agent.epsilon
            },
            'training_data': {
                'episode_rewards': episode_rewards,
                'episode_losses': episode_losses,
                'convergence_data': convergence_data
            }
        }
        
        logger.info(f"{variant_name} è®­ç»ƒå®Œæˆ - "
                  f"æœ€ç»ˆæ€§èƒ½: {results['final_performance']['avg_reward_last_20']:.3f}Â±"
                  f"{results['final_performance']['std_reward_last_20']:.3f}")
        
        return results
    
    def run_full_ablation(self, episodes=100):
        """è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒ"""
        logger.info("å¼€å§‹DDDQN-PERæ¶ˆèå®éªŒ")
        
        # ç»Ÿä¸€é…ç½®
        config = {
            'learning_rate': 1e-4,
            'gamma': 0.99,
            'epsilon': 1.0,
            'epsilon_min': 0.01,
            'epsilon_decay': 0.995,
            'batch_size': 64,
            'buffer_size': 10000,
            'target_update_frequency': 100
        }
        
        # è¿è¡Œæ‰€æœ‰å˜ä½“
        for variant_name in self.variants.keys():
            self.results[variant_name] = self.run_single_variant(variant_name, config, episodes)
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        # ç”Ÿæˆåˆ†æ
        self.analyze_results()
        
        return self.results
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.output_dir / f"ablation_results_{timestamp}.json"
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        serializable_results = {}
        for variant, data in self.results.items():
            serializable_results[variant] = {
                'variant_name': data['variant_name'],
                'description': data['description'],
                'components': data['components'],
                'final_performance': data['final_performance'],
                'training_data': {
                    'episode_rewards': [float(x) for x in data['training_data']['episode_rewards']],
                    'episode_losses': [float(x) for x in data['training_data']['episode_losses']],
                    'convergence_data': data['training_data']['convergence_data']
                }
            }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    
    def analyze_results(self):
        """åˆ†ææ¶ˆèå®éªŒç»“æœ"""
        logger.info("åˆ†ææ¶ˆèå®éªŒç»“æœ...")
        
        # æ€§èƒ½æ±‡æ€»
        performance_summary = {}
        for variant, data in self.results.items():
            perf = data['final_performance']
            performance_summary[variant] = {
                'description': data['description'],
                'components': data['components'],
                'avg_reward': perf['avg_reward_last_20'],
                'std_reward': perf['std_reward_last_20'],
                'best_reward': perf['best_reward']
            }
        
        # æŒ‰æ€§èƒ½æ’åº
        sorted_variants = sorted(performance_summary.items(), 
                               key=lambda x: x[1]['avg_reward'], reverse=True)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(sorted_variants)
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.generate_visualizations()
        
        return performance_summary
    
    def generate_report(self, sorted_variants):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.output_dir / f"ablation_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# DDDQN-PER æ¶ˆèå®éªŒåˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**å®éªŒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ¯ å®éªŒæ¦‚è¿°\n\n")
            f.write("æœ¬æ¶ˆèå®éªŒåˆ†æäº†DDDQN-PERç®—æ³•ä¸­ä¸‰ä¸ªå…³é”®ç»„ä»¶çš„ç‹¬ç«‹è´¡çŒ®ï¼š\n")
            f.write("1. **Dueling Network Architecture**: åˆ†ç¦»çŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿\n")
            f.write("2. **Double DQN**: å‡å°‘Qå€¼è¿‡ä¼°è®¡åå·®\n")
            f.write("3. **Prioritized Experience Replay (PER)**: åŸºäºTDè¯¯å·®çš„é‡è¦æ€§é‡‡æ ·\n\n")
            
            f.write("## ğŸ“Š æ€§èƒ½æ’å\n\n")
            f.write("| æ’å | å˜ä½“åç§° | ç»„ä»¶ | å¹³å‡å¥–åŠ± | æ ‡å‡†å·® | æœ€ä½³å¥–åŠ± |\n")
            f.write("|------|----------|------|----------|--------|----------|\n")
            
            for i, (variant, data) in enumerate(sorted_variants, 1):
                components_str = '+'.join(data['components']) if data['components'] else 'æ— '
                f.write(f"| {i} | {data['description']} | {components_str} | "
                       f"{data['avg_reward']:.3f} | {data['std_reward']:.3f} | {data['best_reward']:.3f} |\n")
            
            f.write("\n## ğŸ” ç»„ä»¶è´¡çŒ®åˆ†æ\n\n")
            
            # è®¡ç®—æ¯ä¸ªç»„ä»¶çš„ç‹¬ç«‹è´¡çŒ®
            baseline = next(data['avg_reward'] for variant, data in sorted_variants 
                          if variant == 'Vanilla_DQN')
            full_model = next(data['avg_reward'] for variant, data in sorted_variants 
                            if variant == 'DDDQN_PER')
            
            f.write(f"### åŸºçº¿æ€§èƒ½\n")
            f.write(f"- **åŸºç¡€DQN**: {baseline:.3f}\n")
            f.write(f"- **å®Œæ•´DDDQN-PER**: {full_model:.3f}\n")
            f.write(f"- **æ€»ä½“æå‡**: {full_model - baseline:.3f} ({((full_model - baseline) / abs(baseline) * 100):.1f}%)\n\n")
            
            # å•ç»„ä»¶è´¡çŒ®
            component_contributions = {}
            for variant, data in sorted_variants:
                if len(data['components']) == 1:
                    component = data['components'][0]
                    improvement = data['avg_reward'] - baseline
                    component_contributions[component] = improvement
                    f.write(f"### {component} ç»„ä»¶è´¡çŒ®\n")
                    f.write(f"- **æ€§èƒ½æå‡**: {improvement:.3f} ({(improvement / abs(baseline) * 100):.1f}%)\n")
                    f.write(f"- **å˜ä½“**: {data['description']}\n\n")
            
            f.write("## ğŸ’¡ å…³é”®å‘ç°\n\n")
            
            # æ‰¾å‡ºæœ€ä½³å•ç»„ä»¶
            if component_contributions:
                best_component = max(component_contributions.items(), key=lambda x: x[1])
                f.write(f"1. **æœ€æœ‰æ•ˆçš„å•ç»„ä»¶**: {best_component[0]} (æå‡ {best_component[1]:.3f})\n")
            
            # ç»„ä»¶ååŒæ•ˆåº”
            total_individual = sum(component_contributions.values())
            actual_improvement = full_model - baseline
            synergy = actual_improvement - total_individual
            
            f.write(f"2. **ç»„ä»¶ååŒæ•ˆåº”**: {synergy:.3f}\n")
            if synergy > 0:
                f.write("   - ç»„ä»¶é—´å­˜åœ¨æ­£å‘ååŒæ•ˆåº”\n")
            elif synergy < 0:
                f.write("   - ç»„ä»¶é—´å­˜åœ¨è´Ÿå‘å¹²æ‰°\n")
            else:
                f.write("   - ç»„ä»¶é—´ç‹¬ç«‹ä½œç”¨\n")
            
            f.write(f"3. **ç®—æ³•å¤æ‚åº¦vsæ€§èƒ½**: éšç€ç»„ä»¶å¢åŠ ï¼Œæ€§èƒ½ç¨³æ­¥æå‡\n\n")
            
            f.write("## ğŸ¯ ç»“è®º\n\n")
            f.write("æ¶ˆèå®éªŒéªŒè¯äº†DDDQN-PERç®—æ³•è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼š\n")
            f.write("- æ¯ä¸ªç»„ä»¶éƒ½å¯¹æœ€ç»ˆæ€§èƒ½æœ‰ç§¯æè´¡çŒ®\n")
            f.write("- ä¸‰ä¸ªç»„ä»¶çš„ç»„åˆå®ç°äº†æœ€ä½³æ€§èƒ½\n")
            f.write("- ç®—æ³•çš„å¤æ‚æ€§æ˜¯åˆç†ä¸”å¿…è¦çš„\n")
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
    
    def generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        logger.info("ç”Ÿæˆæ¶ˆèå®éªŒå¯è§†åŒ–...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig = plt.figure(figsize=(20, 12))
        
        # 1. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        ax1 = plt.subplot(2, 3, 1)
        variants = list(self.results.keys())
        performances = [self.results[v]['final_performance']['avg_reward_last_20'] for v in variants]
        std_devs = [self.results[v]['final_performance']['std_reward_last_20'] for v in variants]
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
        bars = ax1.bar(range(len(variants)), performances, yerr=std_devs, 
                      capsize=5, color=colors, alpha=0.8, edgecolor='black')
        
        ax1.set_xlabel('ç®—æ³•å˜ä½“')
        ax1.set_ylabel('å¹³å‡å¥–åŠ±')
        ax1.set_title('æ¶ˆèå®éªŒæ€§èƒ½å¯¹æ¯”')
        ax1.set_xticks(range(len(variants)))
        ax1.set_xticklabels([self.results[v]['description'] for v in variants], 
                          rotation=45, ha='right')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, perf in zip(bars, performances):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{perf:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. è®­ç»ƒæ”¶æ•›æ›²çº¿
        ax2 = plt.subplot(2, 3, 2)
        for i, (variant, data) in enumerate(self.results.items()):
            convergence = data['training_data']['convergence_data']
            episodes = [c['episode'] for c in convergence]
            avg_rewards = [c['avg_reward'] for c in convergence]
            ax2.plot(episodes, avg_rewards, label=self.results[variant]['description'], 
                    color=colors[i], linewidth=2, marker='o', markersize=4)
        
        ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax2.set_ylabel('æ»‘åŠ¨å¹³å‡å¥–åŠ±')
        ax2.set_title('è®­ç»ƒæ”¶æ•›å¯¹æ¯”')
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax2.grid(True, alpha=0.3)
        
        # 3. ç»„ä»¶è´¡çŒ®åˆ†æ
        ax3 = plt.subplot(2, 3, 3)
        
        # è®¡ç®—åŸºçº¿å’Œå„ç»„ä»¶è´¡çŒ®
        baseline_perf = self.results['Vanilla_DQN']['final_performance']['avg_reward_last_20']
        full_perf = self.results['DDDQN_PER']['final_performance']['avg_reward_last_20']
        
        component_data = {
            'Dueling': self.results['Dueling_DQN']['final_performance']['avg_reward_last_20'] - baseline_perf,
            'Double': self.results['Double_DQN']['final_performance']['avg_reward_last_20'] - baseline_perf,
            'PER': self.results['PER_DQN']['final_performance']['avg_reward_last_20'] - baseline_perf
        }
        
        components = list(component_data.keys())
        contributions = list(component_data.values())
        
        bars = ax3.bar(components, contributions, color=['#FF9999', '#99CCFF', '#99FF99'], 
                      alpha=0.8, edgecolor='black')
        ax3.set_ylabel('æ€§èƒ½æå‡')
        ax3.set_title('å•ç»„ä»¶è´¡çŒ®åˆ†æ')
        ax3.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, contrib in zip(bars, contributions):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.002,
                    f'{contrib:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. æŸå¤±æ”¶æ•›å¯¹æ¯”
        ax4 = plt.subplot(2, 3, 4)
        for i, (variant, data) in enumerate(self.results.items()):
            losses = data['training_data']['episode_losses']
            # å¹³æ»‘æŸå¤±æ›²çº¿
            smoothed_losses = []
            window = 5
            for j in range(len(losses)):
                start_idx = max(0, j - window)
                smoothed_losses.append(np.mean(losses[start_idx:j+1]))
            
            ax4.plot(smoothed_losses, label=self.results[variant]['description'], 
                    color=colors[i], linewidth=2, alpha=0.8)
        
        ax4.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax4.set_ylabel('å¹³å‡æŸå¤±')
        ax4.set_title('æŸå¤±å‡½æ•°æ”¶æ•›å¯¹æ¯”')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. ç»„ä»¶å¤æ‚åº¦vsæ€§èƒ½
        ax5 = plt.subplot(2, 3, 5)
        
        complexity_data = []
        for variant, data in self.results.items():
            complexity = len(data['components'])
            performance = data['final_performance']['avg_reward_last_20']
            complexity_data.append((complexity, performance, data['description']))
        
        complexity_data.sort(key=lambda x: x[0])
        
        complexities = [x[0] for x in complexity_data]
        performances = [x[1] for x in complexity_data]
        labels = [x[2] for x in complexity_data]
        
        scatter = ax5.scatter(complexities, performances, s=100, c=colors[:len(complexity_data)], 
                            alpha=0.7, edgecolors='black')
        
        # æ·»åŠ æ ‡ç­¾
        for i, (x, y, label) in enumerate(complexity_data):
            ax5.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', 
                       fontsize=8, ha='left')
        
        ax5.set_xlabel('ç®—æ³•å¤æ‚åº¦ï¼ˆç»„ä»¶æ•°é‡ï¼‰')
        ax5.set_ylabel('å¹³å‡æ€§èƒ½')
        ax5.set_title('å¤æ‚åº¦vsæ€§èƒ½å…³ç³»')
        ax5.grid(True, alpha=0.3)
        
        # 6. ç¨³å®šæ€§åˆ†æ
        ax6 = plt.subplot(2, 3, 6)
        
        stabilities = [self.results[v]['final_performance']['std_reward_last_20'] for v in variants]
        bars = ax6.bar(range(len(variants)), stabilities, color=colors, alpha=0.8, edgecolor='black')
        
        ax6.set_xlabel('ç®—æ³•å˜ä½“')
        ax6.set_ylabel('æ€§èƒ½æ ‡å‡†å·®')
        ax6.set_title('è®­ç»ƒç¨³å®šæ€§å¯¹æ¯”')
        ax6.set_xticks(range(len(variants)))
        ax6.set_xticklabels([self.results[v]['description'] for v in variants], 
                          rotation=45, ha='right')
        ax6.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, stab in zip(bars, stabilities):
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{stab:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        fig_path = self.output_dir / f"ablation_analysis_{timestamp}.png"
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {fig_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='DDDQN-PERæ¶ˆèå®éªŒ')
    parser.add_argument('--episodes', type=int, default=100, help='è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--output_dir', type=str, default='py/ablation_results', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    experiment = AblationExperiment(args.output_dir)
    
    # è¿è¡Œæ¶ˆèå®éªŒ
    results = experiment.run_full_ablation(args.episodes)
    
    print("\n" + "="*60)
    print("DDDQN-PERæ¶ˆèå®éªŒå®Œæˆ")
    print("="*60)
    
    # æ˜¾ç¤ºç®€è¦ç»“æœ
    performance_summary = {}
    for variant, data in results.items():
        perf = data['final_performance']['avg_reward_last_20']
        performance_summary[variant] = perf
    
    # æŒ‰æ€§èƒ½æ’åº
    sorted_results = sorted(performance_summary.items(), key=lambda x: x[1], reverse=True)
    
    print("\næ€§èƒ½æ’å:")
    for i, (variant, perf) in enumerate(sorted_results, 1):
        description = results[variant]['description']
        components = '+'.join(results[variant]['components']) if results[variant]['components'] else 'æ— '
        print(f"{i}. {description}")
        print(f"   ç»„ä»¶: {components}")
        print(f"   æ€§èƒ½: {perf:.3f}")
        print()
    
    print(f"è¯¦ç»†ç»“æœä¿å­˜åœ¨: {experiment.output_dir}")

if __name__ == "__main__":
    main() 